# 建图阶段通用LLM配置
llm:
  # api_type: "open_llm" # 使用本地vllm
  # base_url: "http://localhost:8002/v1"
  # model: "qwen3-14b"
  # temperature: 0.6
  # chat_template_kwargs: {"enable_thinking": false}  # 禁用Qwen3思考模式
  api_type: "openai" # or open_llm
  base_url: 'https://api2.aigcbest.top/v1'
  model: "deepseek-ai/DeepSeek-V3"
  api_key: "sk"


# 检索专用LLM配置 
retrieval_llm:  
  api_type: "open_llm" # 使用本地vllm
  base_url: "http://localhost:8000/v1"
  model: "qwen3-0.6b"
  max_token: 6144
  MAX_MODEL_LEN: 32768
  temperature: 0.6
  chat_template_kwargs: {"enable_thinking": true}  # 启用Qwen3思考模式

  # api_type: "openai" # or open_llm
  # base_url: 'https://api2.aigcbest.top/v1'
  # model: "qwen3-8b"
  # api_key: "sk"
  # max_token: 6144
  # MAX_MODEL_LEN: 32768
  # temperature: 0.6
  # chat_template_kwargs: {"enable_thinking": true}  # 启用Qwen3思考模式


# 评估专用LLM配置
eval_llm:
  api_type: "openai" # or open_llm
  base_url: 'https://api2.aigcbest.top/v1'
  model: "deepseek-ai/DeepSeek-V3"
  api_key: "sk"
  # max_token: 6144           # ✅ 最大生成长度 (传入API)
  # MAX_MODEL_LEN: 32768     # ✅ 模型最大上下文长度 (用于计算截断)
  # temperature: 0.6          # ✅ 采样温度 (传入API)
  # api_type: "open_llm" # 使用本地vllm
  # base_url: "http://localhost:8003/v1"
  # model: "qwen3-8b"
  # max_token: 6144
  # MAX_MODEL_LEN: 32768
  # temperature: 0.6

embedding:
  api_type: "hf"  # or  ollama / openai.
  # base_url: "https://cfcus02.opapi.win/v1"  # or forward url / other llm url
  api_key: "YOUR_API_KEY"
  model: "../cache/models/modelscope/hub/models/AI-ModelScope/all-MiniLM-L6-v2"
  cache_dir: ""
  dimensions: 384
  max_token_size: 8192
  embed_batch_size: 128
  embedding_func_max_async: 16
  device: "cuda:0"  # 使用CPU: "cpu" 或使用GPU: "cuda:0"
  # target_devices: ["cpu"]  # 可选：明确指定目标设备列表
 
 
data_root:  "./Data" # Root directory for data
working_dir: ./ # Result directory for the experiment
# exp_name: "qwen3-8b-top-5-only-iter"  # Experiment name
exp_name: "xx"  # Experiment name
