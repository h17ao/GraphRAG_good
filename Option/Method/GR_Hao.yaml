################################# Working settings  #################################
# Basic Config
use_entities_vdb: True
use_relations_vdb: True  # Only set True for LightRAG, ToG and GR
llm_model_max_token_size: 32768
use_entity_link_chunk: False  # Only set True for HippoRAG and FastGraphRAG
enable_graph_augmentation: False

# Data
index_name: er_graph
vdb_type: vector  # vector/colbert

# Chunk Config
chunk:
  chunk_token_size: 1200
  chunk_overlap_token_size: 100
  token_model: gpt-3.5-turbo
  chunk_method: chunking_by_token_size

# Graph Config
graph:
  # enable LightRAG
    enable_edge_keywords: True
    graph_type: er_graph # rkg_graph/er_graph/tree_graph/passage_graph
    force: False
    # Building graph
    extract_two_step: True
    max_gleaning: 1
    enable_entity_description: False
    enable_entity_type: False
    enable_edge_description: False
    enable_edge_name: True

# Retrieval Config
retriever:
    query_type: hao
    enable_local: False
    use_entity_similarity_for_ppr: False
    node_specificity: False
    # GR
    top_k: 10
    use_relations_vdb: True

query:
    query_type: qa
    only_need_context: False
    enable_hybrid_query: True
    augmentation_ppr: False
    response_type: Multiple Paragraphs
    # GR
    entities_max_tokens: 2000
    relationships_max_tokens: 2000

    # G-Retriever GR
    top_k: 10
    max_txt_len: 512
    topk_e: 2
    cost_e: 0.5
# For HaoQuery
    max_iterations: 5  # 最大迭代次数
    retrieval_method: gr  # 检索方法选择: ppr, basic, kgp, tog, gr, med, dalk