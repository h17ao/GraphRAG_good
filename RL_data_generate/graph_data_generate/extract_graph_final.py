#!/usr/bin/env python3
"""
ä» hippo_rl_data.json çš„ retrieved_context ä¸­æå–å›¾æ•°æ®
æœ€ç»ˆä¿®å¤ç‰ˆæœ¬ï¼šä½¿ç”¨ä¸GraphRAGç›¸åŒçš„chunk_idè®¡ç®—æ–¹å¼
"""

import json
from hashlib import md5
import logging
import networkx as nx
from typing import List, Dict, Any
from pathlib import Path

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class FinalGraphDataExtractor:
    def __init__(self, graph_data_path: str):
        """åˆå§‹åŒ–å›¾æ•°æ®æå–å™¨"""
        self.graph_data_path = Path(graph_data_path)
        self.graph = None        # NetworkX graph
        
        logger.info(f"âœ… å›¾æ•°æ®æå–å™¨åˆå§‹åŒ–å®Œæˆï¼Œæ•°æ®è·¯å¾„: {graph_data_path}")

    def load_graph(self):
        """åŠ è½½GraphMLå›¾æ•°æ®"""
        try:
            # åŠ è½½GraphMLæ–‡ä»¶
            graphml_path = self.graph_data_path / "graph_storage_nx_data.graphml"
            if graphml_path.exists():
                self.graph = nx.read_graphml(str(graphml_path))
                logger.info(f"âœ… GraphMLæ•°æ®åŠ è½½æˆåŠŸ: {self.graph.number_of_nodes()} ä¸ªèŠ‚ç‚¹, {self.graph.number_of_edges()} æ¡è¾¹")
                return True
            else:
                logger.warning(f"âŒ GraphMLæ–‡ä»¶ä¸å­˜åœ¨: {graphml_path}")
                return False
            
        except Exception as e:
            logger.error(f"âŒ å›¾æ•°æ®åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

    def mdhash_id(self, content: str, prefix: str = "") -> str:
        """
        ä½¿ç”¨ä¸GraphRAGç›¸åŒçš„å“ˆå¸Œè®¡ç®—æ–¹å¼
        """
        return prefix + md5(content.encode()).hexdigest()

    def calculate_chunk_id(self, content: str) -> str:
        """
        æ ¹æ®å†…å®¹è®¡ç®—chunk_idï¼Œä½¿ç”¨ä¸GraphRAGç›¸åŒçš„æ–¹å¼
        """
        return self.mdhash_id(content, prefix="chunk-")

    def find_relations_by_chunks(self, chunk_ids: List[str]) -> List[Dict]:
        """
        æ ¹æ®chunk_idsæŸ¥æ‰¾ç›¸å…³å…³ç³»ï¼ˆä»GraphMLå›¾ä¸­ï¼‰
        """
        if self.graph is None:
            logger.warning("Graphæ•°æ®æœªåŠ è½½")
            return []

        try:
            logger.info(f"ğŸ” æŸ¥æ‰¾ {len(chunk_ids)} ä¸ªchunksçš„ç›¸å…³å…³ç³»")
            
            # ç›´æ¥ä»å›¾ä¸­æŸ¥æ‰¾åŒ…å«chunk_idçš„è¾¹
            relations = []
            edges_data = self.graph.edges(data=True)
            
            chunk_id_set = set(chunk_ids)  # è½¬æ¢ä¸ºé›†åˆä»¥æé«˜æŸ¥æ‰¾æ•ˆç‡
            
            for src, dst, edge_data in edges_data:
                source_id = edge_data.get('source_id', '')
                
                # æ£€æŸ¥è¿™æ¡è¾¹æ˜¯å¦æ¥è‡ªæˆ‘ä»¬çš„ä»»ä½•chunk
                # source_idå¯èƒ½åŒ…å«å¤šä¸ªchunk_idï¼Œç”¨<SEP>åˆ†éš”
                if source_id:
                    source_chunks = source_id.split('<SEP>')
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•chunk_idåŒ¹é…
                    if any(chunk_id in chunk_id_set for chunk_id in source_chunks):
                        # æ„é€ å…³ç³»æ•°æ®
                        relation = {
                            'src_id': edge_data.get('src_id', src),
                            'tgt_id': edge_data.get('tgt_id', dst),
                            'relation_name': edge_data.get('relation_name', 'related_to'),
                            'source_id': source_id,
                            'weight': edge_data.get('weight', 1.0)
                        }
                        relations.append(relation)
            
            logger.info(f"ğŸ“Š æˆåŠŸä»å›¾ä¸­è·å– {len(relations)} ä¸ªå…³ç³»")
            return relations

        except Exception as e:
            logger.error(f"æŸ¥æ‰¾å…³ç³»å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []

    def find_entities_by_relations(self, relations: List[Dict]) -> List[Dict]:
        """
        æ ¹æ®å…³ç³»æŸ¥æ‰¾ç›¸å…³å®ä½“ï¼ˆä»GraphMLå›¾ä¸­ï¼‰
        """
        if not relations or self.graph is None:
            return []

        entity_names = set()
        
        # ä»å…³ç³»ä¸­æå–å®ä½“å
        for relation in relations:
            src_id = relation.get('src_id', '')
            tgt_id = relation.get('tgt_id', '')
            if src_id:
                entity_names.add(src_id)
            if tgt_id:
                entity_names.add(tgt_id)

        logger.info(f"ğŸ“Š ä» {len(relations)} ä¸ªå…³ç³»ä¸­æå–åˆ° {len(entity_names)} ä¸ªå”¯ä¸€å®ä½“å")

        # æŸ¥æ‰¾å®ä½“è¯¦ç»†ä¿¡æ¯
        entities = []
        
        for entity_name in entity_names:
            if entity_name in self.graph:
                node_data = self.graph.nodes[entity_name]
                entity = {
                    'entity_name': entity_name,
                    'entity_type': node_data.get('entity_type', ''),
                    'source_id': node_data.get('source_id', ''),
                    'description': node_data.get('description', ''),
                    'content': node_data.get('content', entity_name)
                }
                entities.append(entity)
            else:
                # å¦‚æœå›¾ä¸­æ²¡æœ‰æ‰¾åˆ°ï¼Œåˆ›å»ºåŸºæœ¬å®ä½“
                entities.append({
                    'entity_name': entity_name,
                    'entity_type': 'unknown'
                })

        logger.info(f"ğŸ“Š æˆåŠŸè·å– {len(entities)} ä¸ªå®ä½“è¯¦æƒ…")
        return entities

    def extract_graph_from_contexts(self, contexts: List[str]) -> Dict[str, Any]:
        """
        ä»retrieved_contextsæå–å›¾æ•°æ®
        """
        logger.info(f"ğŸ” å¼€å§‹å¤„ç† {len(contexts)} ä¸ªä¸Šä¸‹æ–‡")

        # 1. è®¡ç®—chunk_ids
        chunk_ids = []
        for i, context in enumerate(contexts):
            chunk_id = self.calculate_chunk_id(context)
            chunk_ids.append(chunk_id)
            logger.debug(f"ä¸Šä¸‹æ–‡ {i+1}: {chunk_id}")

        logger.info(f"ğŸ“‹ è®¡ç®—å¾—åˆ° {len(chunk_ids)} ä¸ªchunk_ids")

        # 2. æŸ¥æ‰¾ç›¸å…³å…³ç³»
        relations = self.find_relations_by_chunks(chunk_ids)
        logger.info(f"ğŸ“Š æ‰¾åˆ° {len(relations)} ä¸ªç›¸å…³å…³ç³»")

        # 3. æŸ¥æ‰¾ç›¸å…³å®ä½“
        entities = self.find_entities_by_relations(relations)
        logger.info(f"ğŸ“Š æ‰¾åˆ° {len(entities)} ä¸ªç›¸å…³å®ä½“")

        # 4. æ„å»ºå›¾ç»“æ„
        return self._build_graph_structure(entities, relations)

    def _build_graph_structure(self, entities: List[Dict], relations: List[Dict]) -> Dict[str, Any]:
        """
        æ„å»ºå›¾ç»“æ„ï¼šå°†å®ä½“å’Œå…³ç³»è½¬æ¢ä¸ºnodeså’Œedgesæ ¼å¼
        """
        # æ„å»ºå®ä½“IDæ˜ å°„
        entity_id_map = {}
        nodes_list = []
        
        for idx, entity in enumerate(entities):
            entity_id = idx + 1
            entity_name = entity.get('entity_name', f'Entity_{entity_id}')
            entity_id_map[entity_name] = entity_id
            
            nodes_list.append({
                "id": entity_id,
                "entity": entity_name
            })

        # æ„å»ºè¾¹
        edges_list = []
        for relation in relations:
            src_name = relation.get('src_id', '')
            dst_name = relation.get('tgt_id', '')
            relation_name = relation.get('relation_name', 'related_to')
            
            if src_name in entity_id_map and dst_name in entity_id_map:
                edges_list.append({
                    "src": entity_id_map[src_name],
                    "relation": relation_name,
                    "dst": entity_id_map[dst_name]
                })

        logger.info(f"âœ… å›¾ç»“æ„æ„å»ºå®Œæˆ: {len(nodes_list)} ä¸ªèŠ‚ç‚¹, {len(edges_list)} æ¡è¾¹")
        
        return {
            "nodes_list": nodes_list,
            "edges_list": edges_list
        }

    def process_data_file(self, input_file: str, output_file: str, start_idx: int = 0, end_idx: int = None):
        """
        å¤„ç†æ•´ä¸ªæ•°æ®æ–‡ä»¶
        """
        logger.info(f"ğŸ“‚ å¼€å§‹å¤„ç†æ–‡ä»¶: {input_file}")

        # åŠ è½½å›¾æ•°æ®
        if not self.load_graph():
            logger.error("âŒ å›¾æ•°æ®åŠ è½½å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
            return

        # è¯»å–æ•°æ®
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        total_items = len(data)
        logger.info(f"ğŸ“Š æ€»å…± {total_items} æ¡æ•°æ®")

        # ç¡®å®šå¤„ç†èŒƒå›´
        if end_idx is None:
            end_idx = total_items
        end_idx = min(end_idx, total_items)
        start_idx = max(0, start_idx)

        logger.info(f"ğŸ¯ å¤„ç†èŒƒå›´: {start_idx} - {end_idx}")

        # å¤„ç†æ•°æ®
        for i in range(start_idx, end_idx):
            item = data[i]
            question = item.get('question', 'N/A')[:50]
            logger.info(f"\nğŸ”„ å¤„ç†æ¡ç›® {i+1}/{total_items}: {question}...")

            # æå–retrieved_context
            retrieved_context = item.get('retrieved_context', [])

            if not retrieved_context:
                logger.warning(f"æ¡ç›® {i+1} æ²¡æœ‰retrieved_contextï¼Œè·³è¿‡")
                item['graph_data'] = {"nodes_list": [], "edges_list": []}
                continue

            # æå–å›¾æ•°æ®
            try:
                graph_data = self.extract_graph_from_contexts(retrieved_context)
                item['graph_data'] = graph_data

                nodes_count = len(graph_data['nodes_list'])
                edges_count = len(graph_data['edges_list'])
                logger.info(f"âœ… æ¡ç›® {i+1} å®Œæˆ: {nodes_count} å®ä½“, {edges_count} å…³ç³»")

            except Exception as e:
                logger.error(f"âŒ æ¡ç›® {i+1} å¤„ç†å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                item['graph_data'] = {"nodes_list": [], "edges_list": []}

            # æ¯å¤„ç†10æ¡ä¿å­˜ä¸€æ¬¡
            if (i + 1) % 10 == 0:
                logger.info(f"ğŸ’¾ ä¸­é—´ä¿å­˜: å·²å¤„ç† {i + 1} æ¡")
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(data, f, indent=2, ensure_ascii=False)

        # æœ€ç»ˆä¿å­˜
        logger.info(f"ğŸ’¾ æœ€ç»ˆä¿å­˜åˆ°: {output_file}")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

        logger.info("ğŸ‰ å¤„ç†å®Œæˆï¼")

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='ä»retrieved_contextæå–ç°æœ‰å›¾æ•°æ®')
    parser.add_argument('--input', default='/data/workspace/hippo_rl_data.json', 
                       help='è¾“å…¥JSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', default='/data/workspace/hippo_rl_data_with_graph_final.json',
                       help='è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--graph-data', default='/data/workspace/hotpotqa_all/HippoRAG/er_graph_colbert',
                       help='å›¾æ•°æ®ç›®å½•è·¯å¾„')
    parser.add_argument('--start', type=int, default=0, help='å¼€å§‹ç´¢å¼•')
    parser.add_argument('--end', type=int, default=None, help='ç»“æŸç´¢å¼•')
    parser.add_argument('--sample', type=int, default=None, help='ä»…å¤„ç†å‰Næ¡æ•°æ®ï¼ˆç”¨äºæµ‹è¯•ï¼‰')

    args = parser.parse_args()

    # å¦‚æœæŒ‡å®šäº†sampleï¼Œè®¾ç½®end
    if args.sample:
        args.end = min(args.sample, args.end) if args.end else args.sample

    # åˆå§‹åŒ–æå–å™¨
    extractor = FinalGraphDataExtractor(args.graph_data)

    # å¤„ç†æ–‡ä»¶
    extractor.process_data_file(
        input_file=args.input,
        output_file=args.output,
        start_idx=args.start,
        end_idx=args.end
    )

if __name__ == "__main__":
    main()