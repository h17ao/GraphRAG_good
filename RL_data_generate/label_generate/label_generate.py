# ç¦ç”¨æ—¥å¿—è¾“å‡º
import logging
import os
logging.getLogger().setLevel(logging.CRITICAL)
os.environ['LOGURU_LEVEL'] = 'CRITICAL'
# ç¦ç”¨loguruæ—¥å¿—æ–‡ä»¶è¾“å‡º
os.environ['LOGURU_DISABLE'] = 'True'

import argparse
import asyncio
import pandas as pd
from pathlib import Path
import re

from Core.Common.LLM import LLM


def check_output_format_detailed(answer: str) -> tuple[bool, str]:
    """æ£€æŸ¥è¾“å‡ºæ ¼å¼å¹¶è¿”å›è¯¦ç»†é”™è¯¯ä¿¡æ¯"""
    answer = answer.strip()
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ‰€æœ‰å¿…éœ€çš„æ ‡ç­¾
    think_match = re.search(r'<think>(.*?)</think>', answer, re.DOTALL)
    label_match = re.search(r'<label>\s*(able|unable)\s*</label>', answer, re.IGNORECASE)
    
    # å¿…é¡»åŒ…å« <think> æ ‡ç­¾
    if not think_match:
        return False, "ç¼ºå°‘<think>æ ‡ç­¾"
    
    # æ£€æŸ¥thinkæ ‡ç­¾æ˜¯å¦ä¸ºç©º
    think_content = think_match.group(1).strip()
    if not think_content:
        return False, "thinkæ ‡ç­¾ä¸ºç©º"
    
    # å¿…é¡»åŒ…å« <label> æ ‡ç­¾ï¼Œä¸”å†…å®¹ä¸º "able" æˆ– "unable"
    if not label_match:
        return False, "ç¼ºå°‘<label>æ ‡ç­¾"
    
    label_content = label_match.group(1).strip().lower()
    if label_content not in ['able', 'unable']:
        return False, f"labelå†…å®¹æ— æ•ˆ: {label_content}"
    
    # æ£€æŸ¥æ ‡ç­¾é¡ºåºï¼šthinkåº”è¯¥åœ¨labelä¹‹å‰
    think_pos = think_match.start()
    label_pos = label_match.start()
    if think_pos >= label_pos:
        return False, "<think>æ ‡ç­¾åº”è¯¥åœ¨<label>æ ‡ç­¾ä¹‹å‰"
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ ‡ç­¾å¤–çš„é¢å¤–æ–‡æœ¬
    clean_text = answer
    clean_text = re.sub(r'<think>.*?</think>', '', clean_text, flags=re.DOTALL)
    clean_text = re.sub(r'<label>.*?</label>', '', clean_text, flags=re.IGNORECASE)
    clean_text = clean_text.strip()
    
    if clean_text:
        return False, f"åŒ…å«æ ‡ç­¾å¤–çš„é¢å¤–æ–‡æœ¬: {clean_text[:50]}..."
    
    return True, "æ ¼å¼æ­£ç¡®"


def check_output_format(answer: str) -> bool:
    """æ£€æŸ¥è¾“å‡ºæ˜¯å¦éµå®ˆpromptä¸­çš„æ ¼å¼è§„åˆ™"""
    result, _ = check_output_format_detailed(answer)
    return result


def log_regenerate(record_id: int, attempt: int, reason: str, log_path: Path):
    """è®°å½•é‡æ–°ç”Ÿæˆçš„ä¿¡æ¯åˆ°æ—¥å¿—æ–‡ä»¶"""
    timestamp = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
    log_entry = f"[{timestamp}] è®°å½•ID: {record_id}, é‡è¯•æ¬¡æ•°: {attempt}, åŸå› : {reason}\n"
    
    with open(log_path, 'a', encoding='utf-8') as f:
        f.write(log_entry)


def load_existing_results(output_path: Path):
    """åŠ è½½å·²å­˜åœ¨çš„è¾“å‡º jsonl æ–‡ä»¶ï¼Œè¿”å› (è®°å½•åˆ—è¡¨, å·²å¤„ç†æ•°é‡)"""
    if output_path.exists():
        try:
            df = pd.read_json(output_path, lines=True)
            records = df.to_dict("records")
            return records, len(records)
        except Exception:
            # æ–‡ä»¶æŸåæˆ–é jsonl æ ¼å¼ï¼Œé‡æ–°å¼€å§‹
            pass
    return [], 0


def save_results(records, output_path: Path):
    """ä»¥ jsonl æ ¼å¼ä¿å­˜è®°å½•åˆ—è¡¨"""
    df = pd.DataFrame(records)
    df.to_json(output_path, orient="records", lines=True)


async def generate_answers_with_save(records, prompt_template: str, start_idx: int = 0, max_concurrent: int = 50, output_path: Path = None, existing_records: list = None):
    """åˆ†æ‰¹å¹¶å‘å¤„ç†è®°å½•ï¼Œæ¯æ‰¹å®Œæˆåç«‹å³ä¿å­˜"""
    llm = LLM()  # ä½¿ç”¨é»˜è®¤é…ç½®çš„ LLM
    all_results = existing_records.copy() if existing_records else []
    dataset_len = len(records)
    regenerate_log_path = Path("regenerate_label.log")
    
    print(f"ğŸš€ åˆ†æ‰¹å¹¶å‘å¤„ç† {dataset_len} ä¸ªè®°å½•ï¼Œæ¯æ‰¹{max_concurrent}ä¸ª")
    
    # åˆ†æ‰¹å¤„ç†
    for batch_start in range(0, len(records), max_concurrent):
        batch_end = min(batch_start + max_concurrent, len(records))
        batch_records = records[batch_start:batch_end]
        
        print(f"ğŸ“¦ å¤„ç†ç¬¬ {batch_start//max_concurrent + 1} æ‰¹: {batch_start + start_idx + 1} - {batch_end + start_idx}")
        
        # æ¯æ‰¹å†…éƒ¨å¹¶å‘å¤„ç†
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def _process_single(rec: dict, idx_in_batch: int):
            async with semaphore:
                original_idx = start_idx + batch_start + idx_in_batch
                prompt = (
                    prompt_template.replace("{question}", str(rec.get("question", "")))
                    .replace("{retrieved_context}", str(rec.get("retrieved_context", "")))
                    .replace("{gold_answer}", str(rec.get("answer", "")))
                )
                
                max_attempts = 100  # æœ€å¤šé‡è¯•100æ¬¡
                for attempt in range(1, max_attempts + 1):
                    answer = await llm.aask(prompt)
                    
                    is_valid, error_reason = check_output_format_detailed(answer)
                    if is_valid:
                        # æ ¼å¼æ­£ç¡®ï¼Œä½¿ç”¨è¿™ä¸ªç­”æ¡ˆ
                        rec["output_rl"] = answer
                        rec["prompt"] = prompt  # è®°å½•å‘é€ç»™å¤§æ¨¡å‹çš„prompt
                        rec["id"] = original_idx
                        if attempt > 1:
                            print(f"âœ… {original_idx + 1}/{start_idx + dataset_len}: å®Œæˆ (é‡è¯•{attempt-1}æ¬¡åæˆåŠŸ)")
                        else:
                            print(f"âœ… {original_idx + 1}/{start_idx + dataset_len}: å®Œæˆ")
                        return rec
                    else:
                        # æ ¼å¼ä¸æ­£ç¡®ï¼Œè®°å½•è¯¦ç»†æ—¥å¿—
                        reason = f"{error_reason} (ç¬¬{attempt}æ¬¡å°è¯•)"
                        log_regenerate(original_idx, attempt, reason, regenerate_log_path)
                        
                        if attempt < max_attempts:
                            print(f"âš ï¸  {original_idx + 1}/{start_idx + dataset_len}: {error_reason}ï¼Œé‡æ–°ç”Ÿæˆä¸­...")
                        else:
                            print(f"âŒ {original_idx + 1}/{start_idx + dataset_len}: é‡è¯•{max_attempts}æ¬¡åä»ä¸ç¬¦åˆæ ¼å¼ ({error_reason})ï¼Œæ ‡è®°ä¸ºformat wrong")
                            # è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ ‡è®°ä¸ºformat wrong
                            rec["output_rl"] = "format wrong"
                            rec["prompt"] = prompt  # å³ä½¿æ ¼å¼ä¸æ­£ç¡®ä¹Ÿè®°å½•prompt
                            rec["id"] = original_idx
                            return rec
                
                return rec  # ç†è®ºä¸Šä¸ä¼šåˆ°è¾¾è¿™é‡Œ
        
        # æ‰§è¡Œå½“å‰æ‰¹æ¬¡çš„å¹¶å‘æŸ¥è¯¢
        tasks = [_process_single(record, i) for i, record in enumerate(batch_records)]
        batch_results = await asyncio.gather(*tasks)
        
        # å°†å½“å‰æ‰¹æ¬¡ç»“æœåŠ å…¥æ€»ç»“æœ
        all_results.extend(batch_results)
        
        # æ¯æ‰¹å®Œæˆåç«‹å³ä¿å­˜
        if output_path:
            save_results(all_results, output_path)
        
        print(f"ğŸ’¾ ç¬¬ {batch_start//max_concurrent + 1} æ‰¹å®Œæˆï¼Œå·²ä¿å­˜ {len(all_results)} ä¸ªè®°å½•")
    
    return all_results


async def main():
    # ç¡¬ç¼–ç å‚æ•°
    input_path = Path("hippo_results.json")
    prompt_path = Path("prompt_label_generate.txt")
    output_path = Path("hippo_result_answer.json")
    max_concurrent = 50  # æ”¹ä¸º50ï¼Œä¸GraphRAG-newä¿æŒä¸€è‡´

    if not input_path.exists():
        raise FileNotFoundError(f"Input file {input_path} not found")
    if not prompt_path.exists():
        raise FileNotFoundError(f"Prompt template {prompt_path} not found")

    # è¯»å– prompt æ¨¡æ¿
    prompt_template = prompt_path.read_text(encoding="utf-8")

    # è¯»å–è¾“å…¥æ•°æ®ï¼ˆjsonl æˆ–æ™®é€š jsonï¼‰
    try:
        df_input = pd.read_json(input_path, lines=True)
    except ValueError:
        df_input = pd.read_json(input_path)
    input_records = df_input.to_dict("records")

    # å¦‚æœ‰å·²ç”Ÿæˆç»“æœåˆ™æ–­ç‚¹ç»­è·‘
    existing_records, processed_cnt = load_existing_results(output_path)
    remaining_records = input_records[processed_cnt:]

    print(f"[æ•°æ®ç”Ÿæˆ] æ€»è®°å½•æ•°: {len(input_records)}, å·²å¤„ç†: {processed_cnt}, å‰©ä½™: {len(remaining_records)}")

    if remaining_records:
        # åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹å®Œæˆåç«‹å³ä¿å­˜
        final_results = await generate_answers_with_save(
            remaining_records,
            prompt_template,
            start_idx=processed_cnt,
            max_concurrent=max_concurrent,
            output_path=output_path,
            existing_records=existing_records
        )
        print(f"âœ… å…¨éƒ¨å¤„ç†å®Œæˆï¼å…±å¤„ç† {len(remaining_records)} ä¸ªè®°å½•")
    else:
        print("[æ•°æ®ç”Ÿæˆ] æ— éœ€å¤„ç†ï¼Œè¾“å‡ºå·²æ˜¯æœ€æ–°ç‰ˆæœ¬ã€‚")

    print(f"[æ•°æ®ç”Ÿæˆ] ç»“æœå·²ä¿å­˜åˆ° {output_path}")
    print(f"[æ•°æ®ç”Ÿæˆ] é‡æ–°ç”Ÿæˆæ—¥å¿—å·²ä¿å­˜åˆ° regenerate_label.log")


if __name__ == "__main__":
    asyncio.run(main()) 