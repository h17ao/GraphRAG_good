# ç¦ç”¨æ—¥å¿—è¾“å‡º
import logging
import os
logging.getLogger().setLevel(logging.CRITICAL)
os.environ['LOGURU_LEVEL'] = 'CRITICAL'
# ç¦ç”¨loguruæ—¥å¿—æ–‡ä»¶è¾“å‡º
os.environ['LOGURU_DISABLE'] = 'True'

import argparse
import asyncio
import pandas as pd
from pathlib import Path
import re

from Core.Common.LLM import LLM


def check_output_format_detailed(answer: str) -> tuple[bool, str]:
    """æ£€æŸ¥è¾“å‡ºæ ¼å¼å¹¶è¿”å›è¯¦ç»†é”™è¯¯ä¿¡æ¯"""
    answer = answer.strip()
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ‰€æœ‰å¿…éœ€çš„æ ‡ç­¾
    think_match = re.search(r'<think>(.*?)</think>', answer, re.DOTALL)
    query_match = re.search(r'<query>(.*?)</query>', answer, re.DOTALL)
    
    # å¿…é¡»åŒ…å« <think> æ ‡ç­¾
    if not think_match:
        return False, "ç¼ºå°‘<think>æ ‡ç­¾"
    
    # æ£€æŸ¥thinkæ ‡ç­¾æ˜¯å¦ä¸ºç©º
    think_content = think_match.group(1).strip()
    if not think_content:
        return False, "thinkæ ‡ç­¾ä¸ºç©º"
    
    # å¿…é¡»åŒ…å« <query> æ ‡ç­¾
    if not query_match:
        return False, "ç¼ºå°‘<query>æ ‡ç­¾"
    
    # æ£€æŸ¥queryæ ‡ç­¾æ˜¯å¦ä¸ºç©º
    query_content = query_match.group(1).strip()
    if not query_content:
        return False, "queryæ ‡ç­¾ä¸ºç©º"
    
    # æ£€æŸ¥æ ‡ç­¾é¡ºåºï¼šthinkåº”è¯¥åœ¨queryä¹‹å‰
    think_pos = think_match.start()
    query_pos = query_match.start()
    if think_pos >= query_pos:
        return False, "<think>æ ‡ç­¾åº”è¯¥åœ¨<query>æ ‡ç­¾ä¹‹å‰"
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ ‡ç­¾å¤–çš„é¢å¤–æ–‡æœ¬
    clean_text = answer
    clean_text = re.sub(r'<think>.*?</think>', '', clean_text, flags=re.DOTALL)
    clean_text = re.sub(r'<query>.*?</query>', '', clean_text, flags=re.DOTALL)
    clean_text = clean_text.strip()
    
    if clean_text:
        return False, f"åŒ…å«æ ‡ç­¾å¤–çš„é¢å¤–æ–‡æœ¬: {clean_text[:50]}..."
    
    return True, "æ ¼å¼æ­£ç¡®"


def check_output_format(answer: str) -> bool:
    """æ£€æŸ¥è¾“å‡ºæ˜¯å¦éµå®ˆpromptä¸­çš„æ ¼å¼è§„åˆ™"""
    result, _ = check_output_format_detailed(answer)
    return result


def log_regenerate(record_id: int, attempt: int, reason: str, log_path: Path):
    """è®°å½•é‡æ–°ç”Ÿæˆçš„ä¿¡æ¯åˆ°æ—¥å¿—æ–‡ä»¶"""
    timestamp = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
    log_entry = f"[{timestamp}] è®°å½•ID: {record_id}, é‡è¯•æ¬¡æ•°: {attempt}, åŸå› : {reason}\n"
    
    with open(log_path, 'a', encoding='utf-8') as f:
        f.write(log_entry)


def load_existing_results(output_path: Path):
    """åŠ è½½å·²å­˜åœ¨çš„è¾“å‡º jsonl æ–‡ä»¶ï¼Œè¿”å› (è®°å½•åˆ—è¡¨, å·²å¤„ç†æ•°é‡)"""
    if output_path.exists():
        try:
            df = pd.read_json(output_path, lines=True)
            records = df.to_dict("records")
            return records, len(records)
        except Exception:
            # æ–‡ä»¶æŸåæˆ–é jsonl æ ¼å¼ï¼Œé‡æ–°å¼€å§‹
            pass
    return [], 0


def save_results(records, output_path: Path):
    """ä»¥ jsonl æ ¼å¼ä¿å­˜è®°å½•åˆ—è¡¨"""
    df = pd.DataFrame(records)
    df.to_json(output_path, orient="records", lines=True)


async def process_unable_records(records, prompt_template: str, start_idx: int = 0, max_concurrent: int = 50, output_path: Path = None, existing_records: list = None):
    """å¤„ç†labelä¸ºunableçš„è®°å½•ï¼Œè°ƒç”¨APIç”ŸæˆæŸ¥è¯¢"""
    llm = LLM()  # ä½¿ç”¨é»˜è®¤é…ç½®çš„ LLM
    all_results = existing_records.copy() if existing_records else []
    dataset_len = len(records)
    regenerate_log_path = Path("regenerate_query.log")
    
    print(f"ğŸš€ åˆ†æ‰¹å¹¶å‘å¤„ç† {dataset_len} ä¸ªunableè®°å½•ï¼Œæ¯æ‰¹{max_concurrent}ä¸ª")
    
    # åˆ†æ‰¹å¤„ç†
    for batch_start in range(0, len(records), max_concurrent):
        batch_end = min(batch_start + max_concurrent, len(records))
        batch_records = records[batch_start:batch_end]
        
        print(f"ğŸ“¦ å¤„ç†ç¬¬ {batch_start//max_concurrent + 1} æ‰¹: {batch_start + start_idx + 1} - {batch_end + start_idx}")
        
        # æ¯æ‰¹å†…éƒ¨å¹¶å‘å¤„ç†
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def _process_single(rec: dict, idx_in_batch: int):
            async with semaphore:
                original_idx = start_idx + batch_start + idx_in_batch
                prompt = (
                    prompt_template.replace("{question}", str(rec.get("question", "")))
                    .replace("{retrieved_context}", str(rec.get("retrieved_context", "")))
                    .replace("{gold_answer}", str(rec.get("gold_answer", "")))
                    .replace("{supporting_facts}", str(rec.get("supporting_facts", "")))
                )
                
                max_attempts = 20  # æœ€å¤šé‡è¯•20æ¬¡
                for attempt in range(1, max_attempts + 1):
                    answer = await llm.aask(prompt)
                    
                    is_valid, error_reason = check_output_format_detailed(answer)
                    if is_valid:
                        # æ ¼å¼æ­£ç¡®ï¼Œä½¿ç”¨è¿™ä¸ªç­”æ¡ˆ
                        rec["output_final"] = answer
                        rec["prompt"] = prompt  # è®°å½•å‘é€ç»™å¤§æ¨¡å‹çš„prompt
                        if attempt > 1:
                            print(f"âœ… {original_idx + 1}/{start_idx + dataset_len}: å®Œæˆ (é‡è¯•{attempt-1}æ¬¡åæˆåŠŸ)")
                        else:
                            print(f"âœ… {original_idx + 1}/{start_idx + dataset_len}: å®Œæˆ")
                        return rec
                    else:
                        # æ ¼å¼ä¸æ­£ç¡®ï¼Œè®°å½•è¯¦ç»†æ—¥å¿—
                        reason = f"{error_reason} (ç¬¬{attempt}æ¬¡å°è¯•)"
                        log_regenerate(original_idx, attempt, reason, regenerate_log_path)
                        
                        if attempt < max_attempts:
                            print(f"âš ï¸  {original_idx + 1}/{start_idx + dataset_len}: {error_reason}ï¼Œé‡æ–°ç”Ÿæˆä¸­...")
                        else:
                            print(f"âŒ {original_idx + 1}/{start_idx + dataset_len}: é‡è¯•{max_attempts}æ¬¡åä»ä¸ç¬¦åˆæ ¼å¼ ({error_reason})ï¼Œæ ‡è®°ä¸ºformat wrong")
                            # è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ ‡è®°ä¸ºformat wrong
                            rec["output_final"] = "format wrong"
                            rec["prompt"] = prompt  # å³ä½¿æ ¼å¼ä¸æ­£ç¡®ä¹Ÿè®°å½•prompt
                            return rec
                
                return rec  # ç†è®ºä¸Šä¸ä¼šåˆ°è¾¾è¿™é‡Œ
        
        # æ‰§è¡Œå½“å‰æ‰¹æ¬¡çš„å¹¶å‘æŸ¥è¯¢
        tasks = [_process_single(record, i) for i, record in enumerate(batch_records)]
        batch_results = await asyncio.gather(*tasks)
        
        # å°†å½“å‰æ‰¹æ¬¡ç»“æœåŠ å…¥æ€»ç»“æœ
        all_results.extend(batch_results)
        
        # æ¯æ‰¹å®Œæˆåç«‹å³ä¿å­˜
        if output_path:
            save_results(all_results, output_path)
        
        print(f"ğŸ’¾ ç¬¬ {batch_start//max_concurrent + 1} æ‰¹å®Œæˆï¼Œå·²ä¿å­˜ {len(all_results)} ä¸ªè®°å½•")
    
    return all_results


async def main():
    # ç¡¬ç¼–ç å‚æ•°
    input_path = Path("hippo_result.json")
    prompt_path = Path("prompt_query_generate.txt")
    output_path = Path("hippo_result_final.json")
    max_concurrent = 50

    if not input_path.exists():
        raise FileNotFoundError(f"Input file {input_path} not found")
    if not prompt_path.exists():
        raise FileNotFoundError(f"Prompt template {prompt_path} not found")

    # è¯»å– prompt æ¨¡æ¿
    prompt_template = prompt_path.read_text(encoding="utf-8")

    # è¯»å–è¾“å…¥æ•°æ®
    try:
        df_input = pd.read_json(input_path, lines=True)
    except ValueError:
        df_input = pd.read_json(input_path)
    input_records = df_input.to_dict("records")

    print(f"[æŸ¥è¯¢ç”Ÿæˆ] æ€»è®°å½•æ•°: {len(input_records)}")

    # åˆ†ç¦»ableå’Œunableè®°å½•
    able_records = [rec for rec in input_records if rec.get("label") == "able"]
    unable_records = [rec for rec in input_records if rec.get("label") == "unable"]
    
    print(f"[æŸ¥è¯¢ç”Ÿæˆ] ableè®°å½•: {len(able_records)}, unableè®°å½•: {len(unable_records)}")

    # å¤„ç†ableè®°å½•ï¼šç›´æ¥å¤åˆ¶gold_answeråˆ°output_final
    for rec in able_records:
        rec["output_final"] = rec.get("gold_answer", "")
    
    print(f"âœ… ableè®°å½•å¤„ç†å®Œæˆï¼Œç›´æ¥å¤åˆ¶gold_answeråˆ°output_final")

    # æ£€æŸ¥æ˜¯å¦æœ‰å·²ç”Ÿæˆçš„ç»“æœï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
    existing_records, processed_cnt = load_existing_results(output_path)
    
    # åˆå¹¶ableè®°å½•å’Œå·²å¤„ç†çš„è®°å½•
    all_results = able_records.copy()
    
    if existing_records:
        # æ‰¾å‡ºå·²å¤„ç†çš„unableè®°å½•
        existing_unable = [rec for rec in existing_records if rec.get("label") == "unable" and "output_final" in rec]
        processed_unable_ids = {rec.get("id") for rec in existing_unable}
        
        # è¿‡æ»¤å‡ºæœªå¤„ç†çš„unableè®°å½•
        remaining_unable = [rec for rec in unable_records if rec.get("id") not in processed_unable_ids]
        
        # åˆå¹¶å·²å¤„ç†çš„unableè®°å½•
        all_results.extend(existing_unable)
        
        print(f"[æŸ¥è¯¢ç”Ÿæˆ] å·²å¤„ç†unableè®°å½•: {len(existing_unable)}, å‰©ä½™unableè®°å½•: {len(remaining_unable)}")
    else:
        remaining_unable = unable_records

    if remaining_unable:
        # å¤„ç†å‰©ä½™çš„unableè®°å½•
        final_results = await process_unable_records(
            remaining_unable,
            prompt_template,
            start_idx=len(able_records) + len([rec for rec in existing_records if rec.get("label") == "unable"]) if existing_records else len(able_records),
            max_concurrent=max_concurrent,
            output_path=output_path,
            existing_records=all_results
        )
        print(f"âœ… unableè®°å½•å¤„ç†å®Œæˆï¼å…±å¤„ç† {len(remaining_unable)} ä¸ªè®°å½•")
    else:
        # ä¿å­˜æœ€ç»ˆç»“æœ
        save_results(all_results, output_path)
        print("[æŸ¥è¯¢ç”Ÿæˆ] æ— éœ€å¤„ç†unableè®°å½•ï¼Œè¾“å‡ºå·²æ˜¯æœ€æ–°ç‰ˆæœ¬ã€‚")

    print(f"[æŸ¥è¯¢ç”Ÿæˆ] ç»“æœå·²ä¿å­˜åˆ° {output_path}")
    print(f"[æŸ¥è¯¢ç”Ÿæˆ] é‡æ–°ç”Ÿæˆæ—¥å¿—å·²ä¿å­˜åˆ° regenerate_query.log")


if __name__ == "__main__":
    asyncio.run(main()) 