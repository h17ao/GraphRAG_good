#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from Core.GraphRAG import GraphRAG
from Option.Config2 import Config
import argparse
import os
import asyncio
from pathlib import Path
from shutil import copyfile
from Data.QueryDataset import RAGQueryDataset
import pandas as pd
from Core.Utils.Evaluation import Evaluator
from Core.Utils.llm_evaluation import wrapper_llm_evaluation

def check_dirs(opt):
    # For each query, save the results in a separate directory
    result_dir = os.path.join(opt.working_dir, opt.exp_name, "Results")
    # Save the current used config in a separate directory
    config_dir = os.path.join(opt.working_dir, opt.exp_name, "Configs")
    # Save the metrics of entire experiment in a separate directory
    metric_dir = os.path.join(opt.working_dir, opt.exp_name, "Metrics")
    os.makedirs(result_dir, exist_ok=True)
    os.makedirs(config_dir, exist_ok=True)
    os.makedirs(metric_dir, exist_ok=True)
    opt_name = args.opt[args.opt.rindex("/") + 1 :]
    basic_name = args.opt.split("/")[0] + "/Config2.yaml"
    copyfile(args.opt, os.path.join(config_dir, opt_name))
    copyfile(basic_name, os.path.join(config_dir, "Config2.yaml"))
    return result_dir

#   ä¿®æ”¹ä¸ºåŒæ—¶ä¿å­˜contextä¸”å®æ—¶è¯»å†™ä¿å­˜
def load_existing_results(result_dir):
    """æ–­ç‚¹ç»­ä¼ ï¼šåŠ è½½å·²æœ‰ç»“æœ"""
    save_path = os.path.join(result_dir, "results.json")
    if os.path.exists(save_path):
        try:
            all_res = pd.read_json(save_path, lines=True).to_dict('records')
            start_idx = len(all_res)
            print(f"ä»ç¬¬ {start_idx+1} æ¡ç»§ç»­...")
            return start_idx, all_res
        except:
            print("ä»å¤´å¼€å§‹...")
    return 0, []

#   ä¿®æ”¹ä¸ºåŒæ—¶ä¿å­˜contextä¸”å®æ—¶è¯»å†™ä¿å­˜
def save_results(all_res, result_dir):
    """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
    save_path = os.path.join(result_dir, "results.json")
    df = pd.DataFrame(all_res)
    df.to_json(save_path, orient="records", lines=True)
    return save_path

#   ä¿®æ”¹ä¸ºåŒæ—¶ä¿å­˜contextä¸”å®æ—¶è¯»å†™ä¿å­˜
#   æ·»åŠ å¹¶å‘æŸ¥è¯¢å¤„ç†(50å¹¶å‘)
async def wrapper_query(query_dataset, digimon, result_dir, max_concurrent=50):
    """åˆ†æ‰¹å¹¶å‘å¤„ç†queryï¼Œç¡®ä¿æ¯æ‰¹å…¨éƒ¨å®Œæˆå†è¿›è¡Œä¸‹ä¸€æ‰¹"""
    dataset_len = len(query_dataset)
    # æ–­ç‚¹ç»­ä¼ 
    start_idx, all_res = load_existing_results(result_dir)
    remaining_queries = query_dataset[start_idx:]
    
    if not remaining_queries:
        print("æ‰€æœ‰æŸ¥è¯¢å·²å®Œæˆï¼")
        return save_results(all_res, result_dir)
    
    print(f"åˆ†æ‰¹å¹¶å‘å¤„ç† {len(remaining_queries)} ä¸ªæŸ¥è¯¢ï¼Œæ¯æ‰¹{max_concurrent}ä¸ª")
    
    # åˆ†æ‰¹å¤„ç†
    for batch_start in range(0, len(remaining_queries), max_concurrent):
        batch_end = min(batch_start + max_concurrent, len(remaining_queries))
        batch_queries = remaining_queries[batch_start:batch_end]
        
        print(f"å¤„ç†ç¬¬ {batch_start//max_concurrent + 1} æ‰¹: {batch_start + start_idx + 1} - {batch_end + start_idx}")
        
        # æ¯æ‰¹å†…éƒ¨å¹¶å‘å¤„ç†
        semaphore = asyncio.Semaphore(max_concurrent)
        batch_results = []
        
        async def process_single_query(query_item, idx_in_batch):
            async with semaphore:
                original_idx = start_idx + batch_start + idx_in_batch
                result = await digimon.query(query_item["question"])
                query_item["output"] = result["response"]
                query_item["retrieved_context"] = result["context"]
                query_item["id"] = original_idx
                
                # ä¿å­˜HaoQueryçš„è¯¦ç»†ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if "total_iterations" in result:
                    query_item["total_iterations"] = result["total_iterations"]
                if "historical_queries" in result:
                    query_item["historical_queries"] = result["historical_queries"]
                if "output_all" in result:
                    query_item["output_all"] = result["output_all"]
                
                print(f"{original_idx + 1}/{dataset_len}: å®Œæˆ")
                return query_item
        
        # æ‰§è¡Œå½“å‰æ‰¹æ¬¡çš„å¹¶å‘æŸ¥è¯¢
        tasks = [process_single_query(query, i) for i, query in enumerate(batch_queries)]
        batch_results = await asyncio.gather(*tasks)
        
        # å°†å½“å‰æ‰¹æ¬¡ç»“æœåŠ å…¥æ€»ç»“æœ
        all_res.extend(batch_results)
        
        # ä¿å­˜å½“å‰è¿›åº¦
        save_path = save_results(all_res, result_dir)
        print(f"ç¬¬ {batch_start//max_concurrent + 1} æ‰¹å®Œæˆï¼Œå·²ä¿å­˜ {len(all_res)} ä¸ªç»“æœ")
    
    print(f"å…¨éƒ¨å¤„ç†å®Œæˆï¼å…±å¤„ç† {len(remaining_queries)} ä¸ªæŸ¥è¯¢ï¼Œç»“æœå·²ä¿å­˜åˆ°: {save_path}")
    return save_path


async def wrapper_evaluation(path, opt, result_dir):
    from datetime import datetime
    
    # ç”Ÿæˆæ—¶é—´æˆ³ (æœˆæ—¥å°æ—¶åˆ†é’Ÿ)
    timestamp = datetime.now().strftime("%m%d%H%M")
    
    # æ‰§è¡Œæ ‡å‡†è¯„ä¼°
    print(f"ğŸ“ˆ æ‰§è¡Œæ ‡å‡†è¯„ä¼°")
    eval = Evaluator(path, opt.dataset_name)
    res_dict = await eval.evaluate()
    
    # å¸¦æ—¶é—´æˆ³çš„metricsæ–‡ä»¶
    metrics_file = os.path.join(result_dir, f"metrics_{timestamp}.json")
    with open(metrics_file, "w") as f:
        f.write(str(res_dict))
    
    # è·å–æ ‡å‡†è¯„ä¼°ç”Ÿæˆçš„.score.jsonæ–‡ä»¶è·¯å¾„
    score_file = path.replace(".json", "_score.json")
    print(f"æ ‡å‡†è¯„ä¼°å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {score_file}")
    print(f"è¯„ä¼°æŒ‡æ ‡ä¿å­˜åˆ°: {metrics_file}")
    
    # æ‰§è¡ŒLLMè¯„ä¼°
    print(f"æ‰§è¡ŒLLMè¯„ä¼°")
    # ä½¿ç”¨Configä¸­çš„eval_llmé…ç½®è¿›è¡ŒLLMè¯„ä¼°
    await wrapper_llm_evaluation(path, config=opt)
    
    # LLMè¯„ä¼°çš„ç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
    llm_eval_file = os.path.join(result_dir, f"llm_eval_results_{timestamp}.json")
    llm_summary_file = os.path.join(result_dir, f"llm_eval_summary_{timestamp}.json")
    print(f"LLMè¯„ä¼°å®Œæˆï¼Œç»“æœæ–‡ä»¶å·²ç”Ÿæˆ")


if __name__ == "__main__":

    # with open("./book.txt") as f:
    #     doc = f.read()

    parser = argparse.ArgumentParser()
    parser.add_argument("-opt", type=str, help="Path to option YMAL file.")
    parser.add_argument("-dataset_name", type=str, help="Name of the dataset.")
    parser.add_argument("-method_name", type=str, help="Name of the method.", default=None)
    parser.add_argument("--retrieval_model", type=str, help="Override retrieval LLM model name", default=None)
    parser.add_argument("--retrieval_base_url", type=str, help="Override retrieval LLM base_url", default=None)
    parser.add_argument("--exp_name", type=str, help="Override experiment name", default=None)
    args = parser.parse_args()

    # å¦‚æœæ²¡æœ‰æä¾›method_nameï¼Œå°è¯•ä»é…ç½®æ–‡ä»¶è·¯å¾„ä¸­è‡ªåŠ¨æå–
    method_name = args.method_name
    if method_name is None and args.opt:
        # ä»è·¯å¾„å¦‚ "Option/Method/HippoRAG.yaml" ä¸­æå– "HippoRAG"
        opt_path = Path(args.opt)
        if opt_path.parent.name == "Method":  # ç¡®ä¿æ˜¯Methodç›®å½•ä¸‹çš„é…ç½®æ–‡ä»¶
            method_name = opt_path.stem  # å»æ‰æ‰©å±•åçš„æ–‡ä»¶å
            print(f"è‡ªåŠ¨ä»é…ç½®æ–‡ä»¶è·¯å¾„æå–æ–¹æ³•å: {method_name}")

    # å¦‚æœæœ‰exp_nameå‚æ•°ï¼Œå…ˆä¸´æ—¶ä¿®æ”¹ç¯å¢ƒå˜é‡ï¼Œè®©Config.parseä½¿ç”¨æ­£ç¡®çš„exp_name
    original_exp_name = None
    if getattr(args, "exp_name", None):
        # ä¸´æ—¶è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œè®©Config.parseä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„exp_name
        import os
        original_exp_name = os.environ.get("EXP_NAME")
        os.environ["EXP_NAME"] = args.exp_name
        print(f"ä¸´æ—¶è®¾ç½®å®éªŒåç§°ä¸º: {args.exp_name}")

    opt = Config.parse(Path(args.opt), dataset_name=args.dataset_name, method_name=method_name)
    
    # æ¢å¤åŸå§‹ç¯å¢ƒå˜é‡
    if original_exp_name is not None:
        if original_exp_name:
            os.environ["EXP_NAME"] = original_exp_name
        else:
            os.environ.pop("EXP_NAME", None)

    # è¦†ç›–æ£€ç´¢æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
    if getattr(args, "retrieval_model", None):
        if not hasattr(opt, "retrieval_llm") or opt.retrieval_llm is None:
            raise ValueError("é…ç½®ä¸­æœªæ‰¾åˆ° retrieval_llmï¼Œè¯·åœ¨ Option/Config2.yaml ä¸­è®¾ç½®åå†ä½¿ç”¨ --retrieval_model è¦†ç›–")
        # è¦†ç›–æ¨¡å‹å
        opt.retrieval_llm.model = args.retrieval_model
        print(f"è¦†ç›–æ£€ç´¢æ¨¡å‹ä¸º: {opt.retrieval_llm.model}")

    # è¦†ç›–æ£€ç´¢æ¨¡å‹çš„ base_urlï¼ˆå¯é€‰ï¼‰
    if getattr(args, "retrieval_base_url", None):
        if not hasattr(opt, "retrieval_llm") or opt.retrieval_llm is None:
            raise ValueError("é…ç½®ä¸­æœªæ‰¾åˆ° retrieval_llmï¼Œè¯·åœ¨ Option/Config2.yaml ä¸­è®¾ç½®åå†ä½¿ç”¨ --retrieval_base_url è¦†ç›–")
        opt.retrieval_llm.base_url = args.retrieval_base_url
        print(f"è¦†ç›–æ£€ç´¢ base_url ä¸º: {opt.retrieval_llm.base_url}")

    # exp_name å·²åœ¨ Config.parse() é˜¶æ®µé€šè¿‡ç¯å¢ƒå˜é‡å¤„ç†
    digimon = GraphRAG(config=opt)
    result_dir = check_dirs(opt)

    query_dataset = RAGQueryDataset(
        data_dir=os.path.join(opt.data_root, opt.dataset_name)
    )
    corpus = query_dataset.get_corpus()
    # corpus = corpus[:10]

    asyncio.run(digimon.insert(corpus))

    # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›è¡ŒæŸ¥è¯¢å¤„ç†
    results_file = os.path.join(result_dir, "results.json")
    start_idx, existing_results = load_existing_results(result_dir)
    
    if start_idx >= len(query_dataset):
        print(f"æ‰€æœ‰ {len(query_dataset)} æ¡æŸ¥è¯¢å·²å®Œæˆ")
        print(f"è·³è¿‡æ£€ç´¢é˜¶æ®µï¼Œç›´æ¥è¿›å…¥è¯„ä¼°")
        save_path = results_file
    else:
        print(f"å¼€å§‹æ£€ç´¢é˜¶æ®µï¼šä»ç¬¬ {start_idx + 1} æ¡ç»§ç»­ï¼ˆå…± {len(query_dataset)} æ¡ï¼‰")
        save_path = asyncio.run(wrapper_query(query_dataset, digimon, result_dir, max_concurrent=50))

    # æ‰§è¡Œè¯„ä¼°
    print(f"å¼€å§‹è¯„ä¼°é˜¶æ®µ")
    asyncio.run(wrapper_evaluation(save_path, opt, result_dir))
    print("æ ‡å‡†è¯„ä¼°å’ŒLLMè¯„ä¼°å‡å·²å®Œæˆ")

    # for train_item in dataloader:

    # a = asyncio.run(digimon.query("Who is Fred Gehrke?"))

    # asyncio.run(digimon.query("Who is Scrooge?"))
