from Core.Query.BaseQuery import BaseQuery
from Core.Common.Logger import logger
from Core.Common.Constants import Retriever
from Core.Prompt import QueryPrompt
from Core.Prompt.HaoPrompt import (
    REASONING_STEP_PROMPT_QWEN, FORCE_ANSWER_PROMPT_QWEN,
    REASONING_STEP_PROMPT, FORCE_ANSWER_PROMPT,
    NO_CONTEXT_AVAILABLE
)
from Core.Common.Utils import truncate_str_by_token_size
import re
import json
import pandas as pd
from io import StringIO
import os


class HaoQuery(BaseQuery):
    def __init__(self, config, retriever_context):
        super().__init__(config, retriever_context)
        self.max_iterations = getattr(config, 'max_iterations', 5)  # é»˜è®¤æœ€å¤§è¿­ä»£5æ¬¡
        self.retrieval_method = getattr(config, 'retrieval_method', 'ppr')  # é»˜è®¤ä½¿ç”¨PPR
        self.all_model_responses = []  # æ–°å¢ï¼šè®°å½•æ‰€æœ‰æ¨¡å‹å›å¤
        

                    
        # æ ¹æ®æ£€ç´¢æ¨¡å‹ç±»å‹é€‰æ‹©promptæ¨¡æ¿
        self._set_prompt_templates()
        
        # åˆå§‹åŒ–å…·ä½“çš„æ£€ç´¢æ–¹æ³•å®ä¾‹
        self._init_retrieval_method(config, retriever_context)
        
        # é…ç½®ä¸Šä¸‹æ–‡tokené™åˆ¶ - åŸºäºæ£€ç´¢æ¨¡å‹çš„MAX_MODEL_LENè®¡ç®—
        model_max_len = getattr(self.llm.config, 'MAX_MODEL_LEN', None)  # è·å–æ£€ç´¢æ¨¡å‹çš„æœ€å¤§é•¿åº¦
        if model_max_len is None:
            model_max_len = 128000  # é»˜è®¤å€¼ï¼Œé€‚ç”¨äºå¤§å¤šæ•°ç°ä»£LLM
        max_token = getattr(self.llm.config, 'max_token', None)  # è·å–æœ€å¤§ç”Ÿæˆtokenæ•°
        if max_token is None:
            max_token = 28000  # é»˜è®¤å€¼ï¼Œé€‚ç”¨äºå¤§å¤šæ•°LLM
        calculated_max_tokens = model_max_len - max_token - 3000  # MAX_MODEL_LEN - max_token - 3000
        self.max_context_tokens = getattr(config, 'max_context_tokens', calculated_max_tokens)
        logger.info(f"HaoQueryä¸Šä¸‹æ–‡tokené™åˆ¶: {self.max_context_tokens} (æ¨¡å‹MAX_MODEL_LEN: {model_max_len}, max_token: {max_token})")
        
        # åˆå§‹åŒ–æˆªæ–­ç»Ÿè®¡ä¿¡æ¯
        self.total_truncation_time = 0.0  # ç´¯è®¡æˆªæ–­è€—æ—¶
        self.truncation_count = 0  # æˆªæ–­æ¬¡æ•°
        



    
    def _set_prompt_templates(self):
        """æ ¹æ®æ£€ç´¢æ¨¡å‹ç±»å‹è®¾ç½®promptæ¨¡æ¿"""
        model_name = self.llm.config.model.lower() if hasattr(self.llm.config, 'model') else ""
        
        if 'qwen' in model_name:
            # ä½¿ç”¨qwenä¸“ç”¨æ¨¡æ¿
            self.reasoning_prompt = REASONING_STEP_PROMPT_QWEN
            self.force_answer_prompt = FORCE_ANSWER_PROMPT_QWEN
            logger.info(f"æ£€æµ‹åˆ°qwenæ¨¡å‹({model_name})ï¼Œä½¿ç”¨qwenä¸“ç”¨promptæ¨¡æ¿")
        else:
            # ä½¿ç”¨é€šç”¨æ¨¡æ¿
            self.reasoning_prompt = REASONING_STEP_PROMPT
            self.force_answer_prompt = FORCE_ANSWER_PROMPT
            logger.info(f"æ£€æµ‹åˆ°éqwenæ¨¡å‹({model_name})ï¼Œä½¿ç”¨é€šç”¨promptæ¨¡æ¿")
    
    def _init_retrieval_method(self, config, retriever_context):
        """åˆå§‹åŒ–å…·ä½“çš„æ£€ç´¢æ–¹æ³•å®ä¾‹"""
        from Core.Query.QueryFactory import QueryFactory
        
        # åˆ›å»ºå¯¹åº”çš„æŸ¥è¯¢æ–¹æ³•å®ä¾‹ï¼Œç”¨äºè°ƒç”¨å…¶æ£€ç´¢é€»è¾‘
        factory = QueryFactory()
        try:
            self.retrieval_query = factory.get_query(self.retrieval_method, config, retriever_context)
            logger.info(f"HaoQueryä½¿ç”¨ {self.retrieval_method} æ–¹æ³•è¿›è¡Œæ£€ç´¢")
        except KeyError:
            logger.error(f"æœªçŸ¥çš„æ£€ç´¢æ–¹æ³•: {self.retrieval_method}")
            raise ValueError(f"ä¸æ”¯æŒçš„æ£€ç´¢æ–¹æ³•: {self.retrieval_method}")


    
    def _parse_response(self, response, extract_answer_only=False):
        """è§£æLLMå›å¤ï¼Œæå–æ ‡ç­¾å†…å®¹"""
        if extract_answer_only:
            # åªæå–answeræ ‡ç­¾ï¼ˆæ ¼å¼å·²éªŒè¯è¿‡ï¼‰
            match = re.search(r'<answer>(.*?)</answer>', response, re.DOTALL)
            return match.group(1).strip()
        
        # æå–å®Œæ•´çš„æ¨ç†å›å¤ï¼ˆæ ¼å¼å·²éªŒè¯è¿‡ï¼‰
        think = self._extract_tag(response, 'think')
        label = self._extract_tag(response, 'label')
        
        if label.lower() == "able":
            content = self._extract_tag(response, 'answer')
        elif label.lower() == "unable":
            content = self._extract_tag(response, 'query')
        else:
            content = ""
            
        return think, label, content
    
    def _extract_tag(self, text, tag):
        """æå–æŒ‡å®šæ ‡ç­¾çš„å†…å®¹"""
        match = re.search(f'<{tag}>(.*?)</{tag}>', text, re.DOTALL)
        return match.group(1).strip() if match else ""

    def _truncate_contexts(self, all_contexts):
        """æˆªæ–­ç´¯ç§¯çš„ä¸Šä¸‹æ–‡ï¼Œç¡®ä¿ä¸è¶…è¿‡tokené™åˆ¶"""
        if not all_contexts:
            return NO_CONTEXT_AVAILABLE
        
        # ä»TextChunkå¯¹è±¡æˆ–å­—ç¬¦ä¸²ä¸­æå–æ–‡æœ¬å†…å®¹
        context_strings = []
        for context in all_contexts:
            if hasattr(context, 'content'):
                # TextChunkå¯¹è±¡ï¼Œæå–content
                context_strings.append(context.content)
            else:
                # å­—ç¬¦ä¸²æ ¼å¼
                context_strings.append(str(context))
            
        # åˆå¹¶æ‰€æœ‰ä¸Šä¸‹æ–‡å¹¶å»é‡
        context_text = "\n\n".join(list(dict.fromkeys(context_strings)))
        
        # å¦‚æœéœ€è¦ï¼Œè¿›è¡Œtokenæˆªæ–­
        if self.max_context_tokens > 0:
            import time
            from Core.Common.Utils import encode_string_by_tiktoken
            
            # è®°å½•åŸå§‹tokenæ•°
            start_time = time.time()
            original_tokens = len(encode_string_by_tiktoken(context_text))
            
            truncated_text = truncate_str_by_token_size(context_text, self.max_context_tokens)
            
            # è®°å½•æˆªæ–­åtokenæ•°å’Œè€—æ—¶
            if truncated_text != context_text:
                truncated_tokens = len(encode_string_by_tiktoken(truncated_text))
                elapsed_time = time.time() - start_time
                
                # æ›´æ–°ç´¯è®¡ç»Ÿè®¡
                self.truncation_count += 1
                self.total_truncation_time += elapsed_time
                
                logger.info(f"ä¸Šä¸‹æ–‡æˆªæ–­: {original_tokens} tokens -> {truncated_tokens} tokensï¼Œç´¯è®¡è€—æ—¶: {self.total_truncation_time:.3f}s")
            return truncated_text
        
        return context_text

    def _is_valid_reasoning_format(self, response):
        """éªŒè¯æ¨ç†å“åº”æ ¼å¼æ˜¯å¦æ­£ç¡®"""
        try:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…éœ€çš„æ ‡ç­¾
            has_think = bool(re.search(r'<think>(.*?)</think>', response, re.DOTALL))
            has_label = bool(re.search(r'<label>(.*?)</label>', response, re.DOTALL))
            
            if not (has_think and has_label):
                return False
                
            # æ£€æŸ¥labelçš„å€¼æ˜¯å¦æœ‰æ•ˆ
            label_match = re.search(r'<label>(.*?)</label>', response, re.DOTALL)
            if label_match:
                label = label_match.group(1).strip().lower()
                if label == "able":
                    # å¦‚æœæ˜¯ableï¼Œå¿…é¡»æœ‰answeræ ‡ç­¾
                    return bool(re.search(r'<answer>(.*?)</answer>', response, re.DOTALL))
                elif label == "unable":
                    # å¦‚æœæ˜¯unableï¼Œå¿…é¡»æœ‰queryæ ‡ç­¾
                    return bool(re.search(r'<query>(.*?)</query>', response, re.DOTALL))
                else:
                    return False
            
            return False
        except Exception:
            return False

    def _is_valid_force_answer_format(self, response):
        """éªŒè¯å¼ºåˆ¶ä½œç­”å“åº”æ ¼å¼æ˜¯å¦æ­£ç¡®"""
        try:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…éœ€çš„æ ‡ç­¾
            has_think = bool(re.search(r'<think>(.*?)</think>', response, re.DOTALL))
            has_answer = bool(re.search(r'<answer>(.*?)</answer>', response, re.DOTALL))
            
            # å¼ºåˆ¶ä½œç­”å¿…é¡»åŒæ—¶åŒ…å«thinkå’Œansweræ ‡ç­¾
            return has_think and has_answer
        except Exception:
            return False

    async def _reasoning_step(self, query, all_contexts, previous_queries=None, current_iteration=1, local_model_responses=None):
        """ä½¿ç”¨æ¨ç†åŠ©æ‰‹åˆ¤æ–­å½“å‰ç´¯ç§¯çš„æ‰€æœ‰contextæ˜¯å¦è¶³å¤Ÿå›ç­”query"""
        # æ„å»ºå†å²æŸ¥è¯¢ä¿¡æ¯
        history_text = ""
        if previous_queries:
            history_text = f"\nPrevious queries (AVOID repeating):\n" + "\n".join([f"- {pq}" for pq in previous_queries]) + "\n"

        # æ„å»ºå¹¶æˆªæ–­ä¸Šä¸‹æ–‡æ–‡æœ¬
        context_text = self._truncate_contexts(all_contexts)
        
        # ä½¿ç”¨æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©çš„promptæ¨¡æ¿
        prompt = self.reasoning_prompt.format(
            query=query,
            context_text=context_text
        )

        # ä¼ ç»Ÿæ¨ç†è·¯å¾„
        logger.info(f"ç¬¬ {current_iteration} æ¬¡è¿­ä»£: ä½¿ç”¨ä¼ ç»Ÿæ¨ç†æ–¹æ³•")

        # é‡è¯•é€»è¾‘ï¼šæœ€å¤šå°è¯•3æ¬¡
        last_response = ""
        for attempt in range(3):
            try:
                logger.debug(f"æ¨ç†æ­¥éª¤ç¬¬ {attempt + 1}/3 æ¬¡å°è¯•")
                response = await self.llm.aask(msg=prompt)
                last_response = response
                
                # è°ƒè¯•: æ‰“å°ä¼ ç»ŸLLMçš„æ¨ç†å›å¤
                logger.info(f"ğŸ” ä¼ ç»ŸLLMæ¨ç†å›å¤ï¼ˆç¬¬{attempt + 1}æ¬¡å°è¯•ï¼‰:\n{response}")
                
                # éªŒè¯æ ¼å¼æ˜¯å¦æ­£ç¡®
                if self._is_valid_reasoning_format(response):
                    logger.debug(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•æ ¼å¼æ­£ç¡®")
                    
                    # è®°å½•æˆåŠŸçš„attempt
                    if local_model_responses is not None:
                        local_model_responses.append({
                            "step": "reasoning",
                            "iteration": current_iteration,
                            "attempt": attempt + 1,
                            "query": query,
                            "response": response
                        })
                    
                    return self._parse_response(response)
                else:
                    logger.warning(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•æ ¼å¼ä¸æ­£ç¡®ï¼Œå‡†å¤‡é‡è¯•")
                        
            except Exception as e:
                logger.warning(f"æ¨ç†æ­¥éª¤ç¬¬ {attempt + 1} æ¬¡å°è¯•å‡ºé”™: {e}ï¼Œå‡†å¤‡é‡è¯•")
                last_response = str(e)
        
        # 3æ¬¡å°è¯•éƒ½å¤±è´¥ï¼Œè®°å½•æœ€åä¸€æ¬¡å°è¯•
        logger.error("3æ¬¡å°è¯•åä»æ— æ³•è·å¾—æ­£ç¡®æ ¼å¼ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²")
        if local_model_responses is not None:
            local_model_responses.append({
                "step": "reasoning",
                "iteration": current_iteration,
                "attempt": 3,
                "query": query,
                "response": ""
            })
        return ("", "able", "")



    async def _retrieve_relevant_contexts(self, query):
        """
        ä½¿ç”¨é…ç½®çš„æ£€ç´¢æ–¹æ³•è·å–ç›¸å…³ä¸Šä¸‹æ–‡
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            è¿”å›æ£€ç´¢åˆ°çš„contextsåˆ—è¡¨
        """
        logger.debug(f"ä½¿ç”¨ {self.retrieval_method} æ–¹æ³•æ£€ç´¢: {query}")
        
        # è°ƒç”¨ä¼ ç»Ÿçš„æ£€ç´¢é€»è¾‘
        contexts = await self.retrieval_query._retrieve_relevant_contexts(query)
        formatted_contexts = self._format_contexts(contexts)
        
        return formatted_contexts
    
    def _format_contexts(self, contexts):
        """ç»Ÿä¸€æ ¼å¼åŒ–ä¸Šä¸‹æ–‡è¿”å›æ ¼å¼"""
        if contexts is None:
            return []
        elif isinstance(contexts, str):
            return [contexts]  # æŸäº›æ–¹æ³•å¯èƒ½è¿”å›å­—ç¬¦ä¸²
        elif isinstance(contexts, (list, tuple)):
            # ç›´æ¥è¿”å›åˆ—è¡¨ï¼Œä¿æŒTextChunkå¯¹è±¡çš„å®Œæ•´æ€§
            return list(contexts)
        else:
            logger.warning(f"æ£€ç´¢æ–¹æ³• {self.retrieval_method} è¿”å›äº†æ„å¤–çš„æ ¼å¼: {type(contexts)}")
            return [str(contexts)]

    async def query(self, query):
        """é‡å†™queryæ–¹æ³•ï¼Œå®ç°è¿­ä»£æŸ¥è¯¢é€»è¾‘"""
        # ä½¿ç”¨å±€éƒ¨å˜é‡è€Œä¸æ˜¯å®ä¾‹å˜é‡ï¼Œé¿å…å¹¶å‘ç«äº‰
        local_model_responses = []
        
        # è·å–åˆå§‹ä¸Šä¸‹æ–‡
        initial_context = await self._retrieve_relevant_contexts(query)
        
        # æ‰§è¡Œè¿­ä»£æŸ¥è¯¢
        result = await self.generation_qa(query, initial_context, local_model_responses)
        
        # è¿”å›å…¼å®¹æ ¼å¼å’Œæ ¸å¿ƒä¿¡æ¯
        final_answer = result.get("final_answer", "")
        all_contexts = result.get("all_contexts", [])
        
        # è®°å½•æ‰€æœ‰æŸ¥è¯¢çš„æˆªæ–­ç»Ÿè®¡æ€»ç»“
        if self.truncation_count > 0:
            logger.info(f"æŸ¥è¯¢å®Œæˆ - æ‰€æœ‰æŸ¥è¯¢ç´¯è®¡è¿›è¡Œ {self.truncation_count} æ¬¡æˆªæ–­, ç´¯è®¡æˆªæ–­è€—æ—¶: {self.total_truncation_time:.3f}s")
        
        return {
            "response": final_answer,  # ä¿æŒå…¼å®¹æ€§ - main.pyç”¨äºoutputå­—æ®µ
            "context": all_contexts,   # ä¿æŒå…¼å®¹æ€§ - main.pyç”¨äºretrieved_contextå­—æ®µ
            "total_iterations": result.get("total_iterations", 1),
            "historical_queries": result.get("historical_queries", []),
            "output_all": local_model_responses # æ–°å¢ï¼šæ·»åŠ æ‰€æœ‰æ¨¡å‹å›å¤
        }

    async def generation_qa(self, query, context, local_model_responses):
        """è¿­ä»£æŸ¥è¯¢çš„ä¸»è¦é€»è¾‘"""
        if context is None:
            return QueryPrompt.FAIL_RESPONSE
              
        current_query = query
        all_contexts = []
        previous_queries = []  # ç”¨äºä¼ é€’ç»™LLMï¼Œé¿å…é‡å¤å¤±è´¥çš„æŸ¥è¯¢
        all_executed_queries = []  # è®°å½•æ‰€æœ‰æ‰§è¡Œè¿‡çš„æŸ¥è¯¢
        current_iteration = 0
        
        logger.info(f"å¼€å§‹HaoQueryè¿­ä»£æŸ¥è¯¢: {query}")
        
        for iteration in range(self.max_iterations):
            current_iteration = iteration + 1
            logger.info(f"=== è¿­ä»£ {current_iteration}/{self.max_iterations}: {current_query} ===")
            
            # è®°å½•å½“å‰æ‰§è¡Œçš„æŸ¥è¯¢
            all_executed_queries.append(current_query)
            
            # è·å–ä¸Šä¸‹æ–‡
            if iteration == 0:
                current_context = context
            else:
                current_context = await self._retrieve_relevant_contexts(current_query)
            all_contexts.extend(current_context if current_context else [])
            
            # æœ€åä¸€æ¬¡è¿­ä»£ï¼šå¼ºåˆ¶ä½œç­”
            if iteration == self.max_iterations - 1:
                logger.info(f"è¾¾åˆ°æœ€å¤§è¿­ä»£æ•°ï¼Œå¼ºåˆ¶ä½œç­”")
                final_answer = await self._force_answer(query, all_contexts, current_iteration, local_model_responses)
                logger.info(f"å¼ºåˆ¶ä½œç­”å®Œæˆ")
                
                return self._build_result(final_answer, current_iteration, all_contexts, all_executed_queries)
            
            # æ¨ç†åˆ¤æ–­
            if previous_queries:
                logger.debug(f"å†å²æŸ¥è¯¢: {previous_queries}")
                
            think, label, content = await self._reasoning_step(query, all_contexts, previous_queries, current_iteration, local_model_responses)
            
            if label.lower() == "able":
                logger.info(f"ç¬¬ {current_iteration} æ¬¡è¿­ä»£æ‰¾åˆ°ç­”æ¡ˆ")
                return self._build_result(content, current_iteration, all_contexts, all_executed_queries)
                
            elif label.lower() == "unable" and content:
                logger.debug(f"å»ºè®®æ–°æŸ¥è¯¢: {content}")
                previous_queries.append(current_query)  # åªæœ‰å¤±è´¥çš„æŸ¥è¯¢åŠ å…¥previous_queries
                current_query = content
            else:
                logger.warning(f"ç¬¬ {current_iteration} æ¬¡è¿­ä»£æ— æ•ˆ")
                break
        
        # å¼‚å¸¸é€€å‡º
        logger.info(f"å¼‚å¸¸é€€å‡ºï¼Œå¼ºåˆ¶ä½œç­”")
        final_answer = await self._force_answer(query, all_contexts, current_iteration, local_model_responses)
        return self._build_result(final_answer, current_iteration, all_contexts, all_executed_queries)
    
    def _build_result(self, final_answer, total_iterations, all_contexts, historical_queries):
        """æ„å»ºè¿”å›ç»“æœ"""
        return {
            "final_answer": final_answer,
            "total_iterations": total_iterations,
            "all_contexts": all_contexts,
            "historical_queries": historical_queries
        }

    async def _force_answer(self, query, all_contexts, current_iteration=None, local_model_responses=None):
        """å¼ºåˆ¶ä½œç­”æ¨¡å¼ï¼šä½¿ç”¨ç´¯ç§¯çš„æ‰€æœ‰ä¸Šä¸‹æ–‡è®©LLMç›´æ¥å›ç­”"""
        # å‡†å¤‡å¹¶æˆªæ–­ä¸Šä¸‹æ–‡æ–‡æœ¬
        context_text = self._truncate_contexts(all_contexts)
        
        # ä½¿ç”¨æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©çš„promptæ¨¡æ¿
        prompt = self.force_answer_prompt.format(
            query=query,
            context_text=context_text
        )

        # ä¼ ç»Ÿå¼ºåˆ¶ä½œç­”è·¯å¾„
        logger.info("ä½¿ç”¨ä¼ ç»Ÿå¼ºåˆ¶ä½œç­”æ–¹æ³•")

        # é‡è¯•é€»è¾‘ï¼šæœ€å¤šå°è¯•3æ¬¡
        last_response = ""
        for attempt in range(3):
            try:
                logger.debug(f"å¼ºåˆ¶ä½œç­”ç¬¬ {attempt + 1}/3 æ¬¡å°è¯•")
                response = await self.llm.aask(msg=prompt, system_msgs=["You are a helpful assistant."])
                last_response = response
                
                # è°ƒè¯•: æ‰“å°ä¼ ç»ŸLLMçš„å¼ºåˆ¶ä½œç­”å›å¤
                logger.info(f"ğŸ” ä¼ ç»ŸLLMå¼ºåˆ¶ä½œç­”å›å¤ï¼ˆç¬¬{attempt + 1}æ¬¡å°è¯•ï¼‰:\n{response}")
                
                # éªŒè¯æ ¼å¼æ˜¯å¦æ­£ç¡®
                if self._is_valid_force_answer_format(response):
                    logger.debug(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•æ ¼å¼æ­£ç¡®")
                    
                    # è®°å½•æˆåŠŸçš„attempt
                    if local_model_responses is not None:
                        local_model_responses.append({
                            "step": "force_answer",
                            "iteration": current_iteration if current_iteration is not None else self.max_iterations,
                            "attempt": attempt + 1,
                            "query": query,
                            "response": response
                        })
                    
                    return self._parse_response(response, extract_answer_only=True)
                else:
                    logger.warning(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•æ ¼å¼ä¸æ­£ç¡®ï¼Œå‡†å¤‡é‡è¯•")
                    
            except Exception as e:
                logger.warning(f"å¼ºåˆ¶ä½œç­”ç¬¬ {attempt + 1} æ¬¡å°è¯•å‡ºé”™: {e}ï¼Œå‡†å¤‡é‡è¯•")
                last_response = str(e)
        
        # 3æ¬¡å°è¯•éƒ½å¤±è´¥ï¼Œè®°å½•æœ€åä¸€æ¬¡å°è¯•
        logger.error("å¼ºåˆ¶ä½œç­”3æ¬¡å°è¯•åä»æ— æ³•è·å¾—æ­£ç¡®æ ¼å¼ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²")
        if local_model_responses is not None:
            local_model_responses.append({
                "step": "force_answer",
                "iteration": current_iteration if current_iteration is not None else self.max_iterations,
                "attempt": 3,
                "query": query,
                "response": ""
            })
        return ""



    async def generation_summary(self, query, context):
        """HaoQueryä¸“æ³¨äºQAä»»åŠ¡ï¼Œä¸æ”¯æŒæ€»ç»“åŠŸèƒ½"""
        return "HaoQueryä¸“æ³¨äºé—®ç­”ä»»åŠ¡ï¼Œä¸æ”¯æŒæ€»ç»“åŠŸèƒ½"