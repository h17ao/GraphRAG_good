#!/usr/bin/env python3

import subprocess
import sys
import os


# MODEL_PATH = ""
MODEL_PATH = ""  # ç¬¬3ä¸ªepochè®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„
HOST = "0.0.0.0"
PORT = 8001

GPU_MEMORY_UTILIZATION = 0.85  # 4090 24GBæ˜¾å­˜åˆ©ç”¨ç‡ (70%ï¼Œé€‚åº”å…¶ä»–è¿›ç¨‹å ç”¨)
MAX_MODEL_LEN = 32768  # æ‰©å±•ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œæ”¯æŒå¤§è§„æ¨¡æ–‡æ¡£å¤„ç† (32768 Ã— 4)
MAX_NUM_SEQS = 100     # 4090å¹¶å‘æ•°ï¼ˆé€‚ä¸­é…ç½®ï¼‰
MAX_NUM_BATCHED_TOKENS = 256  # 4090æ‰¹å¤„ç†å¤§å°ï¼ˆä¿å®ˆé…ç½®ï¼‰
ENABLE_CHUNKED_PREFILL = True   # åˆ†å—é¢„å¡«å……ï¼ˆå‡å°‘å³°å€¼æ˜¾å­˜ï¼‰
ENABLE_PREFIX_CACHING = False   # æš‚æ—¶å…³é—­å‰ç¼€ç¼“å­˜ï¼ˆå‡å°‘æ˜¾å­˜å ç”¨ï¼‰
BLOCK_SIZE = 16                 # vLLMè¦æ±‚ï¼šå—å¤§å°å¿…é¡»æ˜¯16çš„å€æ•°

os.environ["CUDA_VISIBLE_DEVICES"] = "1,3"


cuda_visible_devices = os.environ.get("CUDA_VISIBLE_DEVICES", "")
if cuda_visible_devices:
    device_list = [d for d in cuda_visible_devices.split(",") if d.strip() != ""]
    tensor_parallel_size = len(device_list)
else:
    tensor_parallel_size = 1


cmd = [
    "python", "-m", "vllm.entrypoints.openai.api_server",
    "--model", MODEL_PATH,
    "--host", HOST,
    "--port", str(PORT),
    "--gpu-memory-utilization", str(GPU_MEMORY_UTILIZATION),
    "--max-model-len", str(MAX_MODEL_LEN),
    "--max-num-seqs", str(MAX_NUM_SEQS),
    "--max-num-batched-tokens", str(MAX_NUM_BATCHED_TOKENS),
    "--trust-remote-code",
    "--served-model-name", "qwen3-14b",
    "--block-size", str(BLOCK_SIZE),         # æ˜¾å­˜å—å¤§å°ä¼˜åŒ–
    "--disable-custom-all-reduce",           # å•å¡ä¼˜åŒ–
    "--max-seq-len-to-capture", str(MAX_MODEL_LEN),  # åºåˆ—æ•è·ä¼˜åŒ–
    # "--rope-scaling", '{"rope_type":"yarn","factor":4.0,"original_max_position_embeddings":32768}',  # RoPEç¼©æ”¾é…ç½®
]

if ENABLE_CHUNKED_PREFILL:
    cmd.append("--enable-chunked-prefill")
if ENABLE_PREFIX_CACHING:
    cmd.append("--enable-prefix-caching")
if tensor_parallel_size > 1:
    cmd += ["--tensor-parallel-size", str(tensor_parallel_size)]
    print(f"[å¤šå¡æ¨¡å¼] å¯ç”¨ --tensor-parallel-size {tensor_parallel_size}")
    print(f"[RTX 4090å››å¡] GPU 0,1,2,3 å¼ é‡å¹¶è¡Œ (24GB*4=96GBæ€»æ˜¾å­˜ï¼Œ90%åˆ©ç”¨ç‡)")
else:
    print("[å•å¡æ¨¡å¼] ä½¿ç”¨RTX 4090 GPU (24GBæ˜¾å­˜ï¼Œ90%åˆ©ç”¨ç‡)")

print("\nğŸ”¥ æ­£åœ¨å¯åŠ¨vLLMæœåŠ¡...")
print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")

try:
    subprocess.run(cmd, check=True)
except subprocess.CalledProcessError as e:
    print(f"âŒ vLLMæœåŠ¡å¯åŠ¨å¤±è´¥: {e}")
    sys.exit(1)
except KeyboardInterrupt:
    print("\nğŸ›‘ æœåŠ¡å·²åœæ­¢")
